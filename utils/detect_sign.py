# -*- coding: utf-8 -*-
"""LoadModelAndPredict_SignLanguageCV.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PKwy71-Wn8kG2Ec_oOQL-5A4hMHyM9Jx

# Projectrealisierung:

**Aufgabe:**
Erstellung einer Applikation mit Kamera Input, die Zeichensprache von nicht-HÃ¶renden / nicht-Sprechenden Personen konvertiert und diese anderen Meeting-Teilnehmern als Untertitel ausgibt. Bereitstellung eines Frontends zu Demonstrationszwecken und einer API, um die Modelle in andere Applikationen wie Teams oder Discord zu integrieren.

how to execute this notebook:
1. Go to https://drive.google.com/file/d/1be8Cai-xqnSQKJQmnVxAuEFmm_xU1N9Z/view?usp=sharing (containing the zip file with the 100 glosses), then add a shortcut to this file in your drive so that it is integrated into your google drive
2. Go to https://drive.google.com/file/d/1EIE3FUYi_hvIxAEqxaxxmrHlE1GDmcND/view?usp=sharing (containing the WASL100.json file), then add a shortcut to this file in your drive
3. Simply run the cells in this notebook

# 1 Sign Language Detection

## 1.1 Imports
"""

import os
import pathlib
import random
# The way this tutorial uses the `TimeDistributed` layer requires TF>=2.10
import shutil
from typing import Tuple

import cv2
import imageio
import mediapipe as mp
import numpy as np
import pandas as pd
import tensorflow as tf
from moviepy.editor import VideoFileClip
from moviepy.video.fx.all import crop
from tensorflow.keras.layers import (BatchNormalization, Conv2D, Dense,
                                     Dropout, Flatten, GlobalAveragePooling2D,
                                     MaxPooling2D, RandomCrop, RandomFlip,
                                     RandomRotation, RandomZoom, ZeroPadding2D,
                                     ZeroPadding3D)
from tensorflow_docs.vis import embed

"""## 1.4 preprocess video"""


def format_frames(frame, output_size):
    """
    Pad and resize an image from a video.

    Args:
      frame: Image that needs to resized and padded.
      output_size: Pixel size of the output frame image.

    Return:
      Formatted frame with padding of specified output size.
    """
    frame = tf.image.convert_image_dtype(frame, tf.float32)
    frame = tf.image.resize_with_pad(frame, *output_size)
    return frame


def frames_from_video_file(video_path, n_frames, output_size=(224, 224), frame_step=15):
    """
    Creates frames from each video file present for each category.

    Args:
      video_path: File path to the video.
      n_frames: Number of frames to be created per video file.
      output_size: Pixel size of the output frame image.

    Return:
      An NumPy array of frames in the shape of (n_frames, height, width, channels).
    """
    # Read each video frame by frame
    result = []
    src = cv2.VideoCapture(str(video_path))

    video_length = src.get(cv2.CAP_PROP_FRAME_COUNT)

    need_length = 1 + (n_frames - 1) * frame_step

    if need_length > video_length:
        start = 0
    else:
        max_start = video_length - need_length
        start = random.randint(0, max_start + 1)

    src.set(cv2.CAP_PROP_POS_FRAMES, start)
    # ret is a boolean indicating whether read was successful, frame is the image itself
    ret, frame = src.read()
    result.append(format_frames(frame, output_size))

    for _ in range(n_frames - 1):
        for _ in range(frame_step):
            ret, frame = src.read()
        if ret:
            frame = format_frames(frame, output_size)
            result.append(frame)
        else:
            result.append(np.zeros_like(result[0]))
    src.release()
    result = np.array(result)[..., [2, 1, 0]]

    return result


"""## 1.5 Create Dataset Generator"""


class FrameGenerator:
    def __init__(self, path, n_frames, training=False):
        """Returns a set of frames with their associated label.

        Args:
          path: Video file paths.
          n_frames: Number of frames.
          training: Boolean to determine if training dataset is being created.
        """
        self.path = path
        self.n_frames = n_frames
        self.training = training
        self.class_names = sorted(
            set(p.name for p in self.path.iterdir() if p.is_dir())
        )
        self.class_ids_for_name = dict(
            (name, idx) for idx, name in enumerate(self.class_names)
        )

    def get_files_and_class_names(self):
        video_paths = list(self.path.glob("*/*.mp4"))
        classes = [p.parent.name for p in video_paths]
        return video_paths, classes

    def __call__(self):
        video_paths, classes = self.get_files_and_class_names()

        pairs = list(zip(video_paths, classes))

        if self.training:
            random.shuffle(pairs)

        for path, name in pairs:
            video_frames = frames_from_video_file(path, self.n_frames)
            label = self.class_ids_for_name[name]  # Encode labels
            yield video_frames, label


# Create the training set and return class names
def create_training_set(video_path_for_generator: pathlib.Path):
    output_signature = (
        tf.TensorSpec(shape=(None, None, None, 3), dtype=tf.float32),
        tf.TensorSpec(shape=(), dtype=tf.int16),
    )
    test_ds = tf.data.Dataset.from_generator(
        FrameGenerator(video_path_for_generator, 10, training=False),
        output_signature=output_signature,
    )

    fg = FrameGenerator(video_path_for_generator, 10, training=True)

    test_ds = test_ds.batch(2)

    test_frames, test_labels = next(iter(test_ds))

    print(f"Shape of validation set of frames: {test_frames.shape}")
    print(f"Shape of validation labels: {test_labels.shape}")

    return test_ds


"""# 2 Modelltraining

## 2.1 Training setup
"""


def preprocess_3d_data(data):
    # Randomly crop the video.
    crop_size = (10, 224, 224, 3)
    data_shape = tf.shape(data)
    data = tf.image.random_crop(
        data,
        size=(data_shape[0], crop_size[0], crop_size[1], crop_size[2], crop_size[3]),
    )

    # Randomly flip the video horizontally.
    data = tf.map_fn(lambda x: tf.image.random_flip_left_right(x), data)

    # Add more data augmentation techniques here if needed.

    return data


class DataAugmentationLayer(tf.keras.layers.Layer):
    def __init__(self, **kwargs):
        super(DataAugmentationLayer, self).__init__(**kwargs)

    def call(self, inputs):
        return preprocess_3d_data(inputs)


def create_preprocessing_layer(input_shape: Tuple = (10, 224, 224, 3), rescaling=True):
    video_input = tf.keras.Input(shape=input_shape)
    x = ZeroPadding3D(padding=((0, 0), (4, 4), (4, 4)))(video_input)
    x = DataAugmentationLayer()(x)
    if rescaling:
        x = tf.keras.layers.Rescaling(scale=255)(x)
    return video_input, x


def create_efficient_net_model(
    base_model_trainable: bool = True, rescaling: bool = True
):
    net = tf.keras.applications.EfficientNetB0(include_top=False)
    net.trainable = base_model_trainable
    # Example usage:
    input_shape = (10, 224, 224, 3)
    video_input, x = create_preprocessing_layer(input_shape, rescaling=rescaling)

    x = tf.keras.layers.TimeDistributed(net)(x)
    x = tf.keras.layers.Dense(100)(x)
    x = tf.keras.layers.GlobalAveragePooling3D()(x)
    return tf.keras.Model(inputs=video_input, outputs=x)


"""# 3 Detection and Recognition with Pre-Trained model

## 3.1 Top 100 classes
"""


def create_class_names():
    return [
        "accident",
        "africa",
        "all",
        "apple",
        "basketball",
        "bed",
        "before",
        "bird",
        "birthday",
        "black",
        "blue",
        "book",
        "bowling",
        "brown",
        "but",
        "can",
        "candy",
        "chair",
        "change",
        "cheat",
        "city",
        "clothes",
        "color",
        "computer",
        "cook",
        "cool",
        "corn",
        "cousin",
        "cow",
        "dance",
        "dark",
        "deaf",
        "decide",
        "doctor",
        "dog",
        "drink",
        "eat",
        "enjoy",
        "family",
        "fine",
        "finish",
        "fish",
        "forget",
        "full",
        "give",
        "go",
        "graduate",
        "hat",
        "hearing",
        "help",
        "hot",
        "how",
        "jacket",
        "kiss",
        "language",
        "last",
        "later",
        "letter",
        "like",
        "man",
        "many",
        "medicine",
        "meet",
        "mother",
        "need",
        "no",
        "now",
        "orange",
        "paint",
        "paper",
        "pink",
        "pizza",
        "play",
        "pull",
        "purple",
        "right",
        "same",
        "school",
        "secretary",
        "shirt",
        "short",
        "son",
        "study",
        "table",
        "tall",
        "tell",
        "thanksgiving",
        "thin",
        "thursday",
        "time",
        "walk",
        "want",
        "what",
        "white",
        "who",
        "woman",
        "work",
        "wrong",
        "year",
        "yes",
    ]


"""## 3.2 Detection Functions

"""


def mediapipe_detection(image, model):
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    image.flags.writeable = False
    results = model.process(image)
    image.flags.writeable = True
    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
    return image, results


def draw_landmarks(image, results):
    mp_holistic = mp.solutions.holistic  # Holistic model
    mp_drawing = mp.solutions.drawing_utils  # Drawing utilities

    # Draw left hand connections
    image_new = mp_drawing.draw_landmarks(
        image,
        landmark_list=results.left_hand_landmarks,
        connections=mp_holistic.HAND_CONNECTIONS,
        landmark_drawing_spec=mp_drawing.DrawingSpec(
            color=(232, 254, 255), thickness=1, circle_radius=4
        ),
        connection_drawing_spec=mp_drawing.DrawingSpec(
            color=(255, 249, 161), thickness=2, circle_radius=2
        ),
    )

    image = image_new if image_new is not None else image
    # Draw right hand connections
    image_new = mp_drawing.draw_landmarks(
        image,
        landmark_list=results.right_hand_landmarks,
        connections=mp_holistic.HAND_CONNECTIONS,
        landmark_drawing_spec=mp_drawing.DrawingSpec(
            color=(232, 254, 255), thickness=1, circle_radius=4
        ),
        connection_drawing_spec=mp_drawing.DrawingSpec(
            color=(255, 249, 161), thickness=2, circle_radius=2
        ),
    )
    image = image_new if image_new is not None else image

    return image


def cut_videos(start_frame, end_frame, video_path, counter, min_x, max_x, min_y, max_y):
    cap_temp = cv2.VideoCapture(video_path)

    output_path = (
        video_path.split(".")[0] + str(counter) + "." + video_path.split(".")[1]
    )

    # Get the frames per second (fps) and frame count of the video
    fps = cap_temp.get(cv2.CAP_PROP_FPS)
    frame_count = cap_temp.get(cv2.CAP_PROP_FRAME_COUNT)

    # Set the start and end frame numbers
    start_frame_num = start_frame
    end_frame_num = end_frame

    # Check if the specified frames are within the video's range
    if start_frame_num > frame_count or end_frame_num > frame_count:
        print("Invalid frame range.")
        cap_temp.release()
        exit()

    # Set the start frame position
    cap_temp.set(cv2.CAP_PROP_POS_FRAMES, start_frame_num)

    # Create a VideoWriter object to write the extracted frames
    fourcc = cv2.VideoWriter_fourcc(*"mp4v")
    output = cv2.VideoWriter(
        output_path, fourcc, fps, (int(cap_temp.get(3)), int(cap_temp.get(4)))
    )

    # Read and write the frames within the specified range
    current_frame = start_frame_num

    while current_frame <= end_frame_num:
        ret, frame = cap_temp.read()
        if not ret:
            break

        height, width = frame.shape[:2]
        # print(height, width)
        # cropped_frame = frame[int(min_x*width):int(max_x*width), int(min_y*height):int(max_y*height)]

        # print(cropped_frame.shape)

        output.write(frame)
        current_frame += 1

    cap_temp.release()

    return {
        output_path: [
            int(min_x * width),
            int(min_y * height),
            int(max_x * width),
            int(max_y * height),
        ]
    }
    # cap.release()
    # clip = VideoFileClip(output_path)
    # new_clip = crop(clip, x1=int(min_x*width), y1=int(min_y*height), x2=int(max_x*width), y2=int(max_y*height))
    # target_path_cropped = output_path.split(".")[0] + "cropped." + output_path.split(".")[1]
    # print(new_clip.write_videofile(target_path_cropped, codec='mpeg4', audio=False))


def filter_min_max(lst):
    x_vals = [i.x for i in lst]
    y_vals = [i.y for i in lst]
    min_x = min(x_vals)
    max_x = max(x_vals)
    min_y = min(y_vals)
    max_y = max(y_vals)
    return [min_x, max_x, min_y, max_y]


def get_min_max_of_face(frame):
    with mp_face_mesh.FaceMesh(
        static_image_mode=True,
        max_num_faces=1,
        #    refine_landmarks=True,
        min_detection_confidence=0.5,
    ) as face_mesh:
        results = face_mesh.process(frame)

    if bool(results.multi_face_landmarks):
        face_landmarks = results.multi_face_landmarks[0]
        face_coordinates = [face_landmark for face_landmark in face_landmarks.landmark]
        return filter_min_max(face_coordinates)
    else:
        return [None, None, None, None]


def create_empty_dataframe():
    return pd.DataFrame(
        columns=[
            "left_hand_x_min",
            "left_hand_x_max",
            "left_hand_y_min",
            "left_hand_y_max",
            "right_hand_x_min",
            "right_hand_x_max",
            "right_hand_y_min",
            "right_hand_y_max",
            "face_x_min",
            "face_x_max",
            "face_y_min",
            "face_y_max",
        ]
    )


def crop_videos_from_dataframe(df: pd.DataFrame, cap):
    # for item in ["left_hand_x_min", "left_hand_x_max", "right_hand_x_min", "right_hand_x_max"]:
    #     df[item].fillna(df["face_x_min"].dropna().max())

    # for item in ["left_hand_y_max", "left_hand_y_min", "right_hand_y_max", "right_hand_y_min"]:
    #     df[item].fillna(df["face_y_min"].dropna().max())

    # df["left_hand_x_max"].fillna(df["face_x_max"].dropna().max())
    min_x = np.nanmin(
        np.array(
            (
                df["left_hand_x_min"].dropna().min(),
                df["face_x_min"].dropna().min(),
                df["right_hand_x_min"].dropna().min(),
            )
        )
    )
    min_y = np.nanmin(
        np.array(
            (
                df["left_hand_y_min"].dropna().min(),
                df["face_y_min"].dropna().min(),
                df["right_hand_y_min"].dropna().min(),
            )
        )
    )
    max_x = np.nanmax(
        np.array(
            (
                df["left_hand_x_max"].dropna().max(),
                df["face_x_max"].dropna().max(),
                df["right_hand_x_max"].dropna().max(),
            )
        )
    )
    max_y = np.nanmax(
        np.array(
            (
                df["left_hand_y_max"].dropna().max(),
                df["face_y_max"].dropna().max(),
                df["right_hand_y_max"].dropna().max(),
            )
        )
    )
    # min_y = min(min(df["left_hand_y_min"]), min(df["face_y_min"]), min(df["right_hand_y_min"]))
    # max_x = max(max(df["left_hand_x_max"]), max(df["face_x_max"]), max(df["right_hand_x_max"]))
    # max_y = max(max(df["left_hand_y_max"]), max(df["face_y_max"]), max(df["right_hand_y_max"]))
    face_middle = (
        df["face_x_min"].dropna().mean() + df["face_x_max"].dropna().mean()
    ) / 2
    distance = max(face_middle - min_x, max_x - face_middle)
    # crop_video(face_middle-distance, min_y, face_middle+distance, max_y)
    min_x = face_middle - distance - 0.1 if face_middle - distance - 0.1 > 0 else 0
    max_x = face_middle + distance + 0.1 if face_middle + distance + 0.1 < 1 else 1
    min_y = min_y - 0.1 if min_y - 0.1 > 0 else 0
    max_y = max_y + 0.1 if max_y + 0.1 < 1 else 1
    print(face_middle - distance, face_middle + distance, min_y, max_y)
    return face_middle - distance, face_middle + distance, min_y, max_y


def predict_detection_of_video(video_file_path):
    cap = cv2.VideoCapture(video_file_path)

    fps = cap.get(cv2.CAP_PROP_FPS)

    threshold_counter_no_hand = fps // 2

    video_times = []
    crop_video = []
    video_positions = create_empty_dataframe()
    start_time_temp = None
    counter_no_hand = None
    # end_time_temp = None
    frame_counter = 0

    # Check if camera opened successfully
    if cap.isOpened() == False:
        print("Error opening video stream or file")

    with mp.solutions.holistic.Holistic(
        min_detection_confidence=0.5, min_tracking_confidence=0.5
    ) as holistic:
        # Read until video is completed
        while cap.isOpened():
            # Capture frame-by-frame
            ret, frame = cap.read()
            if ret == True:
                frame_counter += 1

                # Make detections
                image, results = mediapipe_detection(frame, holistic)
                if results is not None:
                    if results.left_hand_landmarks is not None:
                        video_positions_temp = filter_min_max(
                            [
                                hand_landmarks
                                for hand_landmarks in results.left_hand_landmarks.landmark
                            ]
                        )
                    else:
                        video_positions_temp = [None, None, None, None]
                    if results.right_hand_landmarks is not None:
                        video_positions_temp.extend(
                            filter_min_max(
                                [
                                    hand_landmarks
                                    for hand_landmarks in results.right_hand_landmarks.landmark
                                ]
                            )
                        )
                    else:
                        video_positions_temp.extend([None, None, None, None])

                    video_positions_temp.extend(get_min_max_of_face(frame))
                    video_postions_temp_df = pd.DataFrame(
                        [video_positions_temp],
                        columns=[
                            "left_hand_x_min",
                            "left_hand_x_max",
                            "left_hand_y_min",
                            "left_hand_y_max",
                            "right_hand_x_min",
                            "right_hand_x_max",
                            "right_hand_y_min",
                            "right_hand_y_max",
                            "face_x_min",
                            "face_x_max",
                            "face_y_min",
                            "face_y_max",
                        ],
                    )
                    video_positions = pd.concat(
                        [video_positions, video_postions_temp_df], ignore_index=True
                    )

                if (
                    results.right_hand_landmarks is None
                    and results.left_hand_landmarks is None
                ):
                    if counter_no_hand is not None:
                        counter_no_hand += 1

                        if counter_no_hand > threshold_counter_no_hand:
                            # if end_time_temp is None:
                            if (frame_counter - start_time_temp) > fps:
                                video_times.append((start_time_temp, frame_counter))
                                min_x, max_x, min_y, max_y = crop_videos_from_dataframe(
                                    video_positions, cap
                                )
                                video_positions = create_empty_dataframe()
                                crop_video.append(
                                    cut_videos(
                                        start_time_temp,
                                        frame_counter,
                                        video_file_path,
                                        len(video_times),
                                        min_x,
                                        max_x,
                                        min_y,
                                        max_y,
                                    )
                                )
                                start_time_temp = None
                                counter_no_hand = None

                else:
                    if start_time_temp is None:
                        start_time_temp = frame_counter
                        counter_no_hand = 0

                if results is not None and image is not None:
                    image = draw_landmarks(image, results)

            # Break the loop
            else:
                break

    if start_time_temp is not None:
        if (frame_counter - start_time_temp) > fps:
            video_times.append((start_time_temp, frame_counter))
            min_x, max_x, min_y, max_y = crop_videos_from_dataframe(
                video_positions, cap
            )
            video_positions = create_empty_dataframe()
            crop_video.append(
                cut_videos(
                    start_time_temp,
                    frame_counter,
                    video_file_path,
                    len(video_times),
                    min_x,
                    max_x,
                    min_y,
                    max_y,
                )
            )

    # When everything done, release the video capture object
    cap.release()

    # for item in crop_video:
    #     for key, value in item.items():
    #         clip = VideoFileClip(key)
    #         clip.reader.close()
    #         clip.audio.reader.close_proc()
    #         new_clip = crop(clip, x1=value[0], y1=value[1], x2=value[2], y2=value[3])
    #         cropped_video_dir = os.path.join(os.path.dirname(key), "cropped\\")
    #         if not os.path.exists(cropped_video_dir):
    #             os.mkdir(cropped_video_dir)
    #         target_path_cropped = cropped_video_dir + key.split("\\")[-1]
    #         target_path_cropped = target_path_cropped.replace(
    #             ".mp4", f"_cropped_{value[0]}_{value[1]}_{value[2]}_{value[3]}.mp4"
    #         )
    #         # + "cropped." + key.split(".")[1]
    #         try:
    #             new_clip.write_videofile(target_path_cropped, audio=False)
    #         # new_clip.write_videofile(key, audio=False)

    #         # clip.release()
    #         except Exception as e:
    #             print(e)
    #             new_clip.write_videofile("./test", audio=False)
    #         clip.release()
    for item in crop_video:
        for key, value in item.items():
            # Open the video file
            video = cv2.VideoCapture(key)

            x1 = value[0]
            y1 = value[1]
            x2 = value[2]
            y2 = value[3]

            # Get the video's frame rate and dimensions
            fps = video.get(cv2.CAP_PROP_FPS)
            width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))
            height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))

            cropped_video_dir = os.path.join(os.path.dirname(key), "cropped\\")
            if not os.path.exists(cropped_video_dir):
                os.mkdir(cropped_video_dir)
            target_path_cropped = cropped_video_dir + key.split("\\")[-1]

            # Create a VideoWriter object to write the cropped video
            writer = cv2.VideoWriter(
                target_path_cropped,
                cv2.VideoWriter_fourcc(*"mp4v"),
                fps,
                (x2 - x1, y2 - y1),
            )

            while True:
                # Read a frame from the video
                ret, frame = video.read()

                if not ret:
                    break

                # Crop the frame
                cropped_frame = frame[y1:y2, x1:x2]

                # Write the cropped frame to the video file
                writer.write(cropped_frame)

            # # Release the resources
            # video.release()
            # writer.release()

            # new_clip = crop(clip, x1=value[0], y1=value[1], x2=value[2], y2=value[3])
            # cropped_video_dir = os.path.join(os.path.dirname(key), "cropped\\")
            # if not os.path.exists(cropped_video_dir):
            #     os.mkdir(cropped_video_dir)
            # target_path_cropped = cropped_video_dir + key.split("\\")[-1]
            # target_path_cropped = target_path_cropped.replace(
            #     ".mp4", f"_cropped_{value[0]}_{value[1]}_{value[2]}_{value[3]}.mp4"
            # )
            # # + "cropped." + key.split(".")[1]
            # try:
            #     new_clip.write_videofile(target_path_cropped, audio=False)
            # # new_clip.write_videofile(key, audio=False)

            # # clip.release()
            # except Exception as e:
            #     print(e)
            #     new_clip.write_videofile("./test", audio=False)
            # clip.release()

    return crop_video, video_times, fps, os.path.dirname(key)


"""## 3.3. Load Recognition Model

- Model has to be stored under the root dir in drive
- this can be achieved by copying the model and set the dir to "Meine Ablagen"
"""


def load_recognition_model(
    test_ds, model_path: str = "./utils/training_checkpoints_efficientNet/cp-0021.ckpt"
):
    model = create_efficient_net_model(base_model_trainable=True, rescaling=True)

    model.compile(
        optimizer="adam",
        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
        metrics=["accuracy"],
    )

    model.fit(test_ds.take(1), epochs=1)

    # Loads the weights
    model.load_weights(model_path)

    return model


def to_gif(images):
    converted_images = np.clip(images * 255, 0, 255).astype(np.uint8)
    imageio.mimsave("./animation.gif", converted_images, fps=10)
    return embed.embed_file("./animation.gif")


#
# def recognize_video_folder(video_file_path, model):
#
#     global CLASS_NAMES
#     # Create an instance of FrameGenerator for the chosen dataset
#     chosen_fg = FrameGenerator(pathlib.Path(video_file_path), 10, training=False)
#
#     labels: list = []
#
#     for chosen_frames, true_label in chosen_fg():
#         # Select random video and true label
#         # chosen_frames, true_label = random.choice(list(chosen_fg()))
#
#         # model prediction
#         chosen_frames_expanded = np.expand_dims(chosen_frames, axis=0)
#         predicted_label = np.argmax(model.predict(chosen_frames_expanded), axis=-1)
#
#         # Get the true label's class name
#         true_class_name = chosen_fg.class_names[true_label]
#
#         # get the predicted labels class name
#         # CLASS_NAMES = fg.class_names
#         predicted_class_name = CLASS_NAMES[predicted_label[0]]
#
#         labels.append(true_label)
#
#         print(f"True label: {true_label} ({true_class_name})")
#         print(f"Predicted label: {predicted_label[0]} ({predicted_class_name})")
#
#     return labels


def recognize_video_folder(video_file_path, model):
    chosen_fg = FrameGenerator(pathlib.Path(video_file_path), 10, training=False)

    labels: list = []

    for chosen_frames, true_label in chosen_fg():
        # Select random video and true label
        # chosen_frames, true_label = random.choice(list(chosen_fg()))

        # model prediction
        chosen_frames_expanded = np.expand_dims(chosen_frames, axis=0)
        predicted_label = np.argmax(model.predict(chosen_frames_expanded), axis=-1)

        # Get the true label's class name
        true_class_name = chosen_fg.class_names[true_label]

        # get the predicted labels class name
        # CLASS_NAMES = fg.class_names
        predicted_class_name = CLASS_NAMES[predicted_label[0]]

        labels.append(predicted_class_name)

        print(f"True label: {true_label} ({true_class_name})")
        print(f"Predicted label: {predicted_label[0]} ({predicted_class_name})")
    print(labels)

    return labels


def predict_video_input(video_path, recognition_model):
    crop_video, video_times, fps, video_dir = predict_detection_of_video(video_path)
    # path_first_video = list(crop_video[0].values())[0]
    # type(path_first_video)
    # folder = os.path.dirname(str(path_first_video))
    labels = recognize_video_folder(video_dir, recognition_model)
    predictions = []
    # for counter, label in enumerate(labels):
    #     predictions.append(
    #         {
    #             "label": label,
    #             "video_times": video_times[counter],
    #             "start_time": video_times[counter][0] / fps,
    #             "end_time": video_times[counter][1] / fps,
    #         }
    #     )

    for counter, time_temp in enumerate(video_times):
        label = labels[counter] if len(labels) >= counter else "NaN"
        predictions.append(
            {
                "label": label,
                "video_times": time_temp,
                "start_time": time_temp[0] / fps,
                "end_time": time_temp[1] / fps,
            }
        )

    shutil.rmtree(video_dir + "\\cropped\\")
    print(predictions)
    return predictions


recognition_model = None
CLASS_NAMES = []
mp_face_mesh = None


def setup_prediction_model():
    global recognition_model
    global CLASS_NAMES
    global mp_face_mesh
    print("start loading the model")
    """## 1.2 Setup Test Dir"""
    video_path_for_generator = "./data/test/"
    video_path_for_generator = pathlib.Path(video_path_for_generator)
    train_ds = create_training_set(video_path_for_generator)
    mp_face_mesh = mp.solutions.face_mesh
    CLASS_NAMES = create_class_names()
    recognition_model = load_recognition_model(train_ds)
    print("model loaded successfully")


def predict_one_path(video_path):
    global recognition_model
    return predict_video_input(video_path, recognition_model)


if __name__ == "__main__":
    setup_prediction_model()
    #
    # path_temp = r"C:\Users\Phili\Desktop\Uni Projekte\SignLanguageDetection\App\Demo\philipp_demo3cropped.mp4"
    # print(os.path.dirname(path_temp))
    #
    # print("start loading the model")
    # """## 1.2 Setup Test Dir"""
    # video_path_for_generator = "../data/test/"
    # # video_file_path = video_path_for_generator +
    # # "/content/africa_basketball_sleep.mp4"
    # # video_file_path = "/content/test_video/"
    # video_path_for_generator = pathlib.Path(video_path_for_generator)
    #
    # train_ds = create_training_set(video_path_for_generator)
    # # print(train_ds)
    # mp_face_mesh = mp.solutions.face_mesh

    # for item, label in train_ds:
    #     print(item, label)
    # model_architecture = create_efficient_net_model()

    # CLASS_NAMES = create_class_names()
    # recognition_model = load_recognition_model(train_ds)
    # print("model loaded successfully")

    # video_file_path = r"C:\Users\Phili\Desktop\Uni Projekte\SignLanguageDetection\App\Demo"

    demo_test_path = (
        r"C:\Users\Phili\Desktop\Uni_Projekte\SignLanguageDetection\App\Demo\01387.mp4"
    )
    print(predict_one_path(demo_test_path))

    # print(predict_video_input(demo_test_path, recognition_model))
